---
category: Map
title: Linguistics - Its Learning Map, Knowledge Architecture, and Fundamental Theory
tags: Linguistics
---

## Foundations and Conceptual Framework of Theoretical Linguistics


### Ontology of Linguistics

Language has been conceived in multiple ways: as an innate mental faculty, a social convention, and a biological capacity. Structuralist Ferdinand de Saussure (1916) famously argued that **language (langue)** is neither merely individual speech nor private thought but a public social system of signs; it exists only in the shared conventions of a speech community. In sharp contrast, the generative tradition (Chomsky) treats language as an internal cognitive object. In Chomsky’s view the relevant object of study is the **I‑language** (an individual’s internal grammar) rather than external utterances (“E‑languages”), which are seen as derivative. Philosophers summarize the options as _mentalism_ (language as mental grammar), _platonism_ (language as abstract types or systems), and _nominalism_ (language as sets of utterances in communities). Contemporary linguists often adopt a pluralistic stance: language incorporates mental structures (grammar), social use (communication), and biological underpinnings (human neurocognition). In this view, linguistics is a science that models the abstract rules and representations of language (often as psychological or formal objects) while also describing their physical realizations in speech, writing, and communication across communities.

### Paradigm Shifts and Brief History

The study of language has undergone several major shifts. In the 19th century **comparative philology** dominated: scholars reconstructed proto-languages (e.g. Proto-Indo-European) by systematic sound correspondences across related tongues. This historical-comparative work (stimulated by William Jones’s discovery of Sanskrit’s kinship to Latin/Greek) laid foundations for **historical linguistics** and typology. In the early 20th century, **structuralism** arose. Saussure’s _Cours_ (1916) inaugurated a synchronic view: language is an abstract system (langue) underlying utterances (parole). European schools (Prague, Hjelmslev) and American contemporaries (Boas, Sapir) developed systematic description of language structure (phonemes, grammar) largely independently in each language or family. In the 1930s–50s, American linguist Leonard Bloomfield codified **American structuralism**: he applied rigorous, behaviorist methods to describe phonology and grammar in unfamiliar languages, largely _excluding_ reference to meaning or mental states. In the 1950s–60s, Noam Chomsky led a **generative revolution**: he argued that linguistics should model the innate, rule-based “competence” underlying all possible utterances. His _Syntactic Structures_ (1957) and _Aspects_ (1965) introduced notions like Universal Grammar, competence vs. performance, and recursive phrase structure. This shifted focus from purely descriptive analysis to formal, explanatory theory.

Since the late 20th century, alternative approaches have gained prominence. **Generative grammar** remains influential but now coexists with **functional** and **cognitive** models. Cognitive linguists (Lakoff, Langacker, Talmy, etc.) view language as governed by general cognitive processes rather than a specialized module. **Functional and usage-based** approaches (Halliday’s systemic-functional grammar, Croft’s Functional Discourse Grammar, Construction Grammar, etc.) emphasize language use, meaning, and communicative function. In particular, **usage-based models** (Bybee, Tomasello, Christiansen, etc.) posit that linguistic structure emerges from usage patterns: language is shaped by frequency, analogy, and embodiment rather than solely by abstract rules. Today’s field is characterized by debates and synthesis: researchers explore how innate structures and learned usage interact. For example, recent work advocates a “third-way” in which an innate grammatical component and usage-driven statistics both shape language.

### Academic Knowledge Structure

Modern theoretical linguistics is typically organized by _levels of structure_ within language. Core subfields include **phonetics** (physical articulation and acoustics of speech sounds) and **phonology** (the abstract sound system of a language), **morphology** (the internal structure of words and morphemes), **syntax** (rules governing sentence structure and word order), **semantics** (principles of meaning and interpretation), and **pragmatics** (contextual meaning and language use). As one overview notes, “the fields that are generally considered the core of theoretical linguistics are phonology, morphology, syntax, and semantics” (with phonetics usually treated as foundational background). In practice, university curricula often offer courses or modules in each of these areas. For example:

*   **Phonetics:** Studies how speech sounds are produced, transmitted, and perceived (articulatory/acoustic analysis).
    
*   **Phonology:** Deals with the patterns and systems of sounds (phonemes, features, rules of sound alternation). Phonology abstracts from raw acoustics to model the mental sound inventory and its organization.
    
*   **Morphology:** Concerns how words are built from smaller units (morphemes) and how word forms inflect or derive. Morphology examines processes like affixation, compounding, and the typological distinction between analytic vs. synthetic languages.
    
*   **Syntax:** Analyzes sentence structure: how words combine into phrases and clauses, and what hierarchical configurations (constituency, dependencies) underlie grammatical sentences. Modern syntax includes transformational or generative models (phrase-structure rules, movement), as well as alternatives (dependency grammar, HPSG, etc.).
    
*   **Semantics:** Explores meaning composition: how word meanings combine into phrase meanings (compositionality) and how sentences convey truth-conditional or propositional content. Formal semantics uses tools from logic and set theory to model meaning, while lexical semantics studies word meanings and relations (synonymy, polysemy).
    
*   **Pragmatics:** Focuses on language in context – how speakers use language to convey implied meanings, perform speech acts, and reference discourse entities. Topics include implicature (Grice), reference resolution, discourse cohesion, and socio-pragmatic factors.
    

These subfields interact: for instance, morphosyntax links morphology and syntax (agreement, word order constraints), phonology interacts with morphology (allomorphy conditioned by sound), and semantics/pragmatics together explain how utterances make sense in context. (Specialized fields like typology, historical linguistics, psycholinguistics and computational linguistics often build on these core areas.)

### Core Concepts and Basics

Theoretical linguistics rests on several foundational concepts and distinctions:

*   **Competence vs. Performance:** Chomsky distinguished a speaker’s _competence_ (the internalized knowledge of grammar) from _performance_ (actual language use with errors). Competence is the abstract system; performance is its real-world application. For example, speakers may fail to produce a grammatical sentence due to memory limits, but this does not change what their competence grammar allows.
    
*   **Grammaticality:** Relatedly, linguists distinguish _grammatical_ (well-formed) sentences from _ungrammatical_ ones. Under a competence-based view, a sentence like _“That cats is eating the mouse”_ is ruled ungrammatical because English grammar requires agreement (so “cats” with plural demonstrative _those_). Grammaticality judgments are often elicited by linguists to infer the rules of competence.
    
*   **Recursion:** A key property of human language is _recursive_ structure: linguistic rules can apply to their own output, allowing phrases to nest indefinitely (e.g. “the cat \[that the dog \[that the man fed\] chased\] meowed”). Recursion underlies the infinite generativity of language – the fact that finite means yield an unbounded number of sentences.
    
*   **Phoneme:** The phoneme is the smallest contrastive sound unit. For example, /p/ and /b/ are different phonemes in “pat” vs. “bat.” A phoneme may have multiple **allophones** (contextual variants) that do not change meaning (e.g. the aspirated \.
    
*   **Morpheme:** The morpheme is the smallest grammatical unit of meaning or function. It can be a free word (e.g. _cat_, _walk_) or a bound element (e.g. English plural _-s_, past tense _-ed_). A word like _“cats”_ contains two morphemes: _cat_ \+ _-s_. Morphemes often have **allomorphs**, variant forms conditioned by context (e.g. _-s_, _-es_, or irregular forms all marking plural).
    
*   **Constituency:** In syntax, sentences are analyzed in terms of **constituents** (phrases). For example, “the big dog” is a noun phrase constituent. Constituency tests (substitution, movement) reveal how words group into hierarchical structures, a cornerstone of phrase-structure grammar.
    
*   **Compositionality:** A fundamental semantic principle (Frege’s principle) is that the meaning of a complex expression is determined by the meanings of its parts and the way they are combined. This explains how we understand novel sentences: we compute the sense of an unfamiliar sentence by combining known word meanings according to grammatical rules.
    
*   **Reference and Meaning:** Linguists distinguish _reference_ (the real-world entities or concepts a term refers to) from _sense_ (conceptual meaning). For example, the word “dog” refers to the set of all dogs but has a sense that distinguishes it from “cat.” Semantics also deals with relations like synonymy, antonymy, and entailment.
    
*   **Universals and Typology:** Linguists seek _universals_ – properties or parameters common to all languages – and use typological classification to organize languages by structural patterns (word order, morphological type, phoneme inventories, etc.). Iconic universals (Greenbergian universals) or functional tendencies (e.g. Subject–Object–Verb order) have been extensively catalogued, reflecting deep constraints or preferences in human language.
    
*   **Markedness:** This concept (originating in Prague School phonology) captures asymmetry: in a contrasting pair, one element is _unmarked_ (the default) and the other _marked_ (more complex or specific). For example, singular _“cat”_ is unmarked, while plural _“cats”_ is marked (by the suffix). Markedness has been applied to sounds (voiced vs. voiceless), morphology (tense distinctions), and syntax (active vs. passive), often correlating with cognitive or frequency factors.
    

These and other core ideas – e.g. _feature geometry_ in phonology, _parameter-setting_ in syntax, _frame semantics_ in lexicon – form the toolkit of theoretical linguists. Mastery of these concepts and the ability to apply them within formal models (rules, tree diagrams, logical formulas) is essential for advanced study.

### Conclusion and Further Topics

The frontier of theoretical linguistics spans many intersecting domains. **Linguistic typology and universals** continue to broaden our understanding of language diversity and limits (with large-scale databases like WALS). **Language acquisition** (first language, second language) connects theory to development: how children infer grammar from input is a major research focus, engaging psycholinguistics and computational modeling. **Neurolinguistics** probes the brain basis of language (e.g. Broca’s and Wernicke’s areas for production/comprehension, neural oscillations in syntax) using imaging and lesion studies. In **formal semantics**, researchers develop richer models (possible worlds, situation semantics) and interfaces with pragmatics (e.g. modeling implicatures and presuppositions).

Important debates persist: _nativist_ versus _usage-based_ accounts of grammar (Chomsky vs. cognitive-functionalists), the modularity of mind versus embodied cognition, and the balance between formal description and communicative function. Recent work, for instance, argues that both innate structure and statistical learning contribute to language. In **philosophy of language**, questions of reference, truth, and meaning (from Frege and Tarski to contemporary pragmatic theories) remain central to linguistic semantics and grammar.

Emerging tools and domains are reshaping the field. **Corpus linguistics** and computational methods allow empirically-driven discoveries across millions of utterances; **machine learning** and NLP raise new questions about modeling competence. **Evolutionary linguistics** investigates how language may have emerged biologically (genes like FOXP2, vocal tract changes, comparative primate studies). **Interdisciplinary work** ties linguistics to genetics, neuroscience, anthropology, and artificial intelligence. Together, these topics ensure theoretical linguistics remains a vibrant field, continually refining its models of what language is and how it works at all levels of structure and use.

### 语言学的本体论（Ontology of Linguistics）

语言曾以多种方式被理解：作为一种先天的心智能力、一种社会约定，以及一种生物学能力。结构主义者费迪南·德·索绪尔（Ferdinand de Saussure，1916）著名地指出，**语言（langue）**既不仅仅是个体言语，也不是私人思想，而是一个公共的、社会性的符号系统；它只存在于言语共同体所共享的约定之中。与之形成鲜明对照的是，生成传统（乔姆斯基）将语言视为一种内部的认知对象。在乔姆斯基看来，相关的研究对象是**I-语言（I-language）**（个体的内部语法），而不是外在的言语产出（“E-语言”），后者被视为派生的结果。哲学家通常将这些立场概括为：*心灵主义（mentalism）*（语言作为心智语法）、*柏拉图主义（platonism）*（语言作为抽象类型或系统）、以及_唯名论（nominalism）_（语言作为共同体中话语集合）。当代语言学家往往采取一种多元立场：语言同时包含心智结构（语法）、社会使用（交际）以及生物学基础（人类神经认知）。在此观点下，语言学是一门科学：它既为语言的抽象规则与表征建模（通常将其视为心理对象或形式对象），同时也描述这些规则在言语、书写以及跨共同体交流中的物理实现方式。

### 范式转变与简史（Paradigm Shifts and Brief History）

语言研究经历了若干重大转变。19世纪，**比较语文学（comparative philology）**占据主导：学者通过相关语言之间系统性的语音对应关系，重建原始语言（例如原始印欧语）。这种历史—比较研究（受到威廉·琼斯发现梵语与拉丁语/希腊语亲缘关系的推动）为**历史语言学**与类型学奠定了基础。20世纪初，**结构主义（structuralism）**兴起。索绪尔的《教程》（*Cours*，1916）确立了共时视角：语言是支撑言语（parole）的抽象系统（langue）。欧洲诸学派（布拉格学派、Hjelmslev等）与美国同时代学者（Boas、Sapir）在很大程度上分别在各自语言或语系范围内，发展出对语言结构（音位、语法）的系统描写。1930—50年代，美国语言学家伦纳德·布卢姆菲尔德（Leonard Bloomfield）规范化了**美国结构主义**：他运用严格的行为主义方法来描写陌生语言的音系与语法，并在很大程度上_排除_对意义或心理状态的诉诸。1950—60年代，诺姆·乔姆斯基（Noam Chomsky）引领了**生成革命（generative revolution）**：他主张语言学应当对支撑一切可能话语的、先天的、基于规则的“能力（competence）”进行建模。他的《句法结构》（*Syntactic Structures*，1957）与《语言理论的方面》（*Aspects*，1965）引入了诸如普遍语法（Universal Grammar）、能力与表现（performance）的区分、以及递归的短语结构等概念。这使研究重心从纯粹的描写分析转向形式化的、解释性的理论。

自20世纪末以来，替代性路径逐渐走向显著。**生成语法（generative grammar）**仍然具有影响力，但如今与**功能主义（functional）**与**认知（cognitive）**模型并存。认知语言学家（Lakoff、Langacker、Talmy等）认为语言主要受一般认知过程支配，而非由某个专门的语言模块所控制。**功能主义与使用基础（usage-based）**路径（如Halliday的系统功能语法、Croft的功能语篇语法、构式语法等）强调语言使用、意义与交际功能。特别地，**使用基础模型（usage-based models）**（Bybee、Tomasello、Christiansen等）主张：语言结构从使用模式中涌现；语言更多地受频率、类比与具身经验所塑形，而不只是由抽象规则单方面决定。当今领域的特征是持续的争论与综合：研究者探索先天结构与习得性使用如何相互作用。例如，近期研究倡导一种“第三道路”，认为先天的语法成分与使用驱动的统计规律共同塑造语言。

### 学术知识结构（Academic Knowledge Structure）

现代理论语言学通常按语言内部的_结构层级（levels of structure）_来组织。核心分支包括：**语音学（phonetics）**（言语声音的物理发音与声学属性）与**音系学（phonology）**（语言的抽象音系系统）、**形态学（morphology）**（词与语素的内部结构）、**句法学（syntax）**（支配句子结构与语序的规则）、**语义学（semantics）**（意义与解释的原则）、以及**语用学（pragmatics）**（语境中的意义与语言使用）。正如某一概述所言：“通常被认为是理论语言学核心的领域是音系学、形态学、句法学与语义学”（而语音学通常被视为更基础的背景）。在实际教学中，大学课程体系往往在这些领域分别设课或设模块。例如：

* **语音学（Phonetics）**：研究言语声音如何被产生、传播与感知（发音学/声学分析）。

* **音系学（Phonology）**：研究声音的模式与系统（音位、特征、音变规则）。音系学从原始声学信号中抽象出来，以建模心智中的语音单位库及其组织方式。

* **形态学（Morphology）**：研究词如何由更小单位（语素）构成，以及词形如何发生屈折变化或派生生成。形态学考察诸如加缀、复合等过程，以及分析语与综合语等类型学区分。

* **句法学（Syntax）**：分析句子结构：词如何组合成短语与分句，以及哪些层级配置（成分结构、依存关系）支撑语法句。现代句法包含转换/生成模型（短语结构规则、移位等），也包含替代框架（依存语法、HPSG等）。

* **语义学（Semantics）**：探究意义的组合：词义如何组合成短语义（组合性），句子如何表达真值条件或命题内容。形式语义学使用逻辑与集合论工具来建模意义；词汇语义学研究词义及其关系（同义、多义等）。

* **语用学（Pragmatics）**：关注语境中的语言——说话者如何通过语言传达隐含意义、实施言语行为，并指称话语中的实体。议题包括会话含义（Grice）、指称消解、语篇衔接，以及社会—语用因素。

这些分支彼此交互：例如，形态句法连接形态与句法（如一致关系、语序限制），音系与形态相互作用（受语音条件制约的语素变体），语义/语用共同解释话语在语境中如何获得可理解性。（类型学、历史语言学、心理语言学与计算语言学等专门领域通常也建立在这些核心领域之上。）

### 核心概念与基础（Core Concepts and Basics）

理论语言学建立在若干基础概念与关键区分之上：

* **能力（Competence）与表现（Performance）**：乔姆斯基区分说话者的_能力_（内化的语法知识）与_表现_（包含错误的实际语言使用）。能力是抽象系统；表现是该系统在现实世界中的应用。例如，说话者可能因记忆限制而未能产出一个语法句，但这并不改变其能力语法所允许的结构范围。

* **语法性（Grammaticality）**：与此相关，语言学家区分_语法的_（良构的）句子与_不语法的_句子。在以能力为基础的视角下，像 *“That cats is eating the mouse”* 这样的句子被判为不语法，因为英语语法要求一致关系（因此“cats”应与复数指示词 *those* 搭配）。语言学家常通过语法性判断来推断能力系统的规则。

* **递归（Recursion）**：人类语言的一个关键属性是_递归_结构：语言规则可以作用于自身的输出，使短语得以无限嵌套（例如 “the cat [that the dog [that the man fed] chased] meowed”）。递归支撑语言的无限生成性——即有限手段产生无界句子的事实。

* **音位（Phoneme）**：音位是最小的对立性语音单位。例如 /p/ 与 /b/ 在 “pat” 与 “bat” 中构成不同音位。一个音位可以有多个 **音位变体（allophones）**（由语境触发的变体），它们不改变意义（例如，在英语中 “pin” 的送气 [pʰ] 与 “spin” 的不送气 [p]）。【译注：此处为对原文截断处的常见补全】

* **语素（Morpheme）**：语素是最小的语法意义或功能单位。它可以是自由形式（如 *cat*, *walk*），也可以是黏着形式（如英语复数 *-s*、过去时 *-ed*）。像 *“cats”* 这样的词包含两个语素：*cat* + *-s*。语素常有 **语素变体（allomorphs）**：由语境制约的形式变体（例如 *-s*, *-es*，或不规则形式都可标记复数）。

* **成分性（Constituency）**：在句法中，句子被分析为由**成分（constituents）**（短语）构成。例如，“the big dog” 是一个名词短语成分。成分测试（替换、移位等）揭示词如何组成层级结构，这是短语结构语法的基石。

* **组合性（Compositionality）**：一个基本语义原则（弗雷格原则）是：复杂表达式的意义由其组成部分的意义以及它们的组合方式所决定。这解释了我们如何理解新句：我们依据语法规则，将已知词义组合起来，从而计算陌生句子的意义。

* **指称与意义（Reference and Meaning）**：语言学家区分_指称（reference）*（某术语指向的现实实体或概念）与_意涵/概念意义（sense）*（概念层面的意义）。例如，“dog” 指称所有狗的集合，但其 sense 使其区别于“cat”。语义学还处理同义、反义与蕴涵等关系。

* **共性与类型学（Universals and Typology）**：语言学家寻求_语言共性（universals）_——所有语言共有的性质或参数——并用类型学分类来组织语言的结构模式（语序、形态类型、音位库存等）。诸如Greenberg式共性或功能倾向（例如主语—宾语—动词语序）已被广泛编目，反映了人类语言的深层约束或偏好。

* **标记性（Markedness）**：这一概念（源自布拉格学派音系学）刻画对立中的不对称性：在一对对比项中，一项是_无标记（unmarked）*（默认项），另一项是_有标记（marked）*（更复杂或更具体）。例如，单数 *“cat”* 常被视为无标记，而复数 *“cats”* 通过后缀形成有标记项。标记性已被用于语音（浊/清对立）、形态（时态区分）与句法（主动/被动）等层面，且常与认知或频率因素相关。

这些以及其他核心思想——例如音系中的_特征几何（feature geometry）*、句法中的_参数设定（parameter-setting）*、词汇中的_框架语义学（frame semantics）_——构成理论语言学家的工具箱。掌握这些概念并能在形式模型（规则、树状图、逻辑公式）中加以运用，是进一步深入研究的必要条件。

### 结论与进一步主题（Conclusion and Further Topics）

理论语言学的前沿横跨多个相互交织的领域。**语言类型学与语言共性**不断拓展我们对语言多样性与边界的理解（借助WALS等大型数据库）。**语言习得**（第一语言、第二语言）将理论与发展联系起来：儿童如何从输入中推断语法是核心问题之一，涉及心理语言学与计算建模。**神经语言学**通过成像与损伤研究探查语言的大脑基础（例如布罗卡区、韦尼克区在产出/理解中的作用，以及句法处理中的神经振荡）。在**形式语义学**中，研究者发展更丰富的模型（可能世界语义、情境语义）并建构与语用学的接口（例如对会话含义与预设的建模）。

重要争论仍在持续：语法的_先天主义（nativist）_解释与_使用基础（usage-based）_解释（乔姆斯基 vs 认知—功能学派）、心智的模块性与具身认知之争，以及形式描写与交际功能之间如何平衡。近期研究例如主张先天结构与统计学习都对语言形成作出贡献。在**语言哲学**中，关于指称、真值与意义的问题（从弗雷格、塔尔斯基到当代语用理论）仍是语言语义与语法研究的核心。

新工具与新领域正在重塑学科。**语料库语言学**与计算方法使得在数百万话语中进行经验驱动的发现成为可能；**机器学习**与自然语言处理（NLP）也提出了关于如何建模能力的新问题。**进化语言学**探究语言如何在生物学上可能出现（如FOXP2等基因、声道变化、灵长类比较研究）。**跨学科研究**将语言学与遗传学、神经科学、人类学与人工智能联结起来。总体而言，这些议题确保理论语言学保持活力，并持续在所有结构层级与使用层面上精炼其关于“语言是什么、如何运作”的模型。

## Casual Chain of the Development of Linguistics

Below is a compact “question → solution/argument → next question” chain (with approximate dates). It is selective (you could build parallel chains for Chinese philology, Arabic grammar, indigenous-language documentation, etc.), but it covers the standard academic narrative plus a few major alternatives.


### 1) Rule-system grammar (ancient South Asia; mid–1st millennium BCE)

**Question:** How can a prestigious language variety be specified *exactly* (so it can be taught, reproduced, and kept stable)?
**Resolution (method):** A highly formal rule system with meta-rules that *generates/licences* well-formed forms (a compact algorithmic grammar). 
**Associated argument/move:** Treat grammar as an explicit rule calculus with economy and coverage as design pressures. 
**Next question produced:** If grammar can be formal, are there *general* principles shared across languages, or links between grammar and thought?


### 2) General/rational grammar (Europe; 1660s)

**Question:** What aspects of grammar reflect universal structures of thought/logic rather than accidents of a particular language?
**Resolution (method):** “General and rational” grammar: analyze grammar through predication/logic and cross-linguistic comparison (Latin/Greek/French, etc.). 
**Associated argument/move:** Similarities across languages arise because there is (roughly) one logic underlying thought. 
**Next question produced:** But languages *change* and diverge—how do we explain historical relatedness and regular change?


### 3) Comparative–historical linguistics and the “sound law” program (Europe; 19th century; “Neogrammarians” late 1800s)

**Question:** Can language change be studied with the rigor of a science (regularities, prediction, falsification)?
**Resolution (method):** Comparative method plus the Neogrammarian regularity hypothesis: sound change is (in principle) exceptionless; apparent exceptions are handled via conditioning environments and analogy. 
**Associated argument/move:** Treat sound change as lawlike; separate phonetic regularity from morphological leveling (analogy). 
**Next question produced:** Historical rigor is not enough—what is the proper object of study for describing a language *as a system* at a time?


### 4) Structuralism (Europe; early 20th century; Saussurean turn)

**Question:** What is “language” as an object—individual utterances, or an abstract system enabling them? How should we separate synchrony (system-at-a-time) from diachrony (change)?
**Resolution (method):** Define the linguistic object as a structured system (langue) distinct from speech (parole); make synchronic structure primary as a scientific target. 
**Associated argument/move:** The sign and the network of relations are central; structure is not reducible to a list of words. 
**Next question produced:** How do we build *replicable* methods to discover structure from data—especially for underdescribed languages?


### 5) American structuralism / distributionalism (U.S.; 1930s–1950s)

**Question:** Can linguistic analysis be made operational and data-driven without relying on introspection or meaning (which seemed “unscientific” to some)?
**Resolution (method):** Distributional analysis and “discovery procedure” ideals: infer categories/structures from observable patterns of occurrence. 
**Associated argument/move:** Syntax can be grounded in distributional facts; meaning can be bracketed (at least initially) to keep procedures objective. 
**Next question produced:** But humans produce and understand *novel* sentences—what kind of internal system explains productivity and acquisition?


### 6) Generative grammar (1957 onward)

**Question:** How can finite resources yield an unbounded set of grammatical sentences, and what must a learner “bring” to acquisition?
**Resolution (method):** A grammar is a generative device that produces all and only the grammatical sequences; model competence as an internal system. 
**Associated argument/move:** Shift from “discovery procedures” to explicit theories evaluated by explanatory adequacy (including learnability considerations). 
**Next question produced:** How do meaning and use connect to syntax—especially quantification, context-dependence, and inference beyond truth conditions?


### 7) Formal semantics (1970s; Montague)

**Question:** Can natural language meaning be treated with the same formal precision as logic, compositionally, including quantification?
**Resolution (method):** Model natural-language syntax–semantics with higher-order logic and lambda calculus; treat natural and formal languages in a unified framework. 
**Associated argument/move:** Reject a sharp theoretical divide between NL and formal systems (as a research stance); insist on compositionality and explicit models. 
**Next question produced:** Meaning in interaction often exceeds compositional truth conditions—how do actions, intentions, and conversational reasoning enter the theory?


### 8) Pragmatics: speech acts + implicature (1960s–1970s; Austin, Grice)

**Question:** What is it to *do* things with words (promise, baptize, order), and how do hearers infer unstated content reliably?
**Resolution (method):** Speech act framework (felicity conditions, illocutionary force) and conversational implicature via cooperative principles/maxims. 
**Associated argument/move:** Not all meaning is truth-conditional; inference is systematic and partly rule-governed by interactional norms. 
**Next question produced:** If use and inference matter, then variation across speakers and social settings is not “noise”—how is it structured, and how does it drive change?


### 9) Variationist sociolinguistics (1960s onward; Labovian program)

**Question:** Is “free variation” real, or is variation patterned by social and linguistic constraints—and does it reveal change in progress?
**Resolution (method):** Quantitative analysis of sociolinguistic variables correlated with social factors; model variation as structured rather than accidental. 
**Associated argument/move:** Empirical field methods + statistics can turn variation into evidence about grammar, diffusion, and change. 
**Next question produced:** Beyond single communities, what constraints and tendencies hold across languages globally (typology/universals), and what explains them?


### 10) Typology and universals (mid-20th century onward; Greenbergian tradition)

**Question:** What cross-linguistic regularities exist (e.g., word-order correlations), and are they accidental, functional, or cognitive?
**Resolution (method):** Large comparative sampling; formulate implicational universals and correlations from attested diversity. 
**Associated argument/move:** Universals can be empirical generalizations over many languages (not only theory-internal claims). 
**Next question produced:** If grammar is shaped by function and use, can we model grammar as a resource for meaning-in-context rather than primarily as formal derivation?


### 11) Functional / systemic-functional linguistics (mid–late 20th century; Halliday)

**Question:** How does language realize social action and meaning across contexts (register/genre), not merely structure?
**Resolution (method):** Model language as a social semiotic system organized around choices (systems), with context-sensitive functional dimensions. 
**Associated argument/move:** Prioritize meaning and function (ideational/interpersonal/textual) as explanatory for linguistic form. 
**Next question produced:** If usage and function matter, are “rules” the right primitives—or are learned form–meaning pairings (“constructions”) and frequency effects more basic?


### 12) Construction grammar, cognitive and usage-based approaches (1980s–2000s+)

**Question:** How are grammar and meaning linked at the level of recurring patterns, and how do frequency and experience shape representation and learning?
**Resolution (method):** Constructions as form–meaning pairings (including argument structure); conceptual metaphor as part of cognition; usage-based learning with frequency effects; usage-based acquisition theories. 
**Associated argument/move:** Grammar emerges from use and general cognition rather than requiring a sharply separate autonomous module (strong versions vary by author). 
**Next question produced:** How can we formalize gradient well-formedness and competing pressures, especially in phonology?

### 13) Constraint-based optimization in phonology (1990s; Optimality Theory)

**Question:** How can we model surface patterns as the result of competing constraints, including cross-linguistic variation?
**Resolution (method):** Ranked, violable constraints evaluated over candidate outputs (optimization). 
**Associated argument/move:** Replace many ordered rules with constraint interaction + language-specific rankings. 
**Next question produced:** With expanding data and computation, how do we scale evidence and evaluation beyond hand-picked examples?

### 14) Corpus linguistics (late 20th century onward)

**Question:** How can linguistic claims be tested and discovered using large-scale real usage data?
**Resolution (method):** Computer-aided analysis of large corpora; corpus evidence used to test, refine, or sometimes drive generalizations. 
**Associated argument/move:** Methodological shift toward reproducibility and broad coverage; stronger emphasis on variation, frequency, collocation, and distribution. 
**Next question produced:** Can computational models learn linguistic structure/meaning from data at scale—and what does that imply about theory?

### 15) Neural NLP and the “foundation model” era (2017– )

**Question:** Can a general architecture learn high-quality language representations and transfer across tasks, and what mechanisms support scaling?
**Resolution (method):** Transformer architecture based on attention; large-scale pretraining (e.g., BERT) and task adaptation. 
**Associated argument/move:** Replace heavy task-specific feature engineering with representation learning and scaling laws; attention enables parallelism and long-range dependencies. 
**Next question produced (current cycle):** What, if anything, do such models *know* in linguistically interpretable terms (syntax/semantics/pragmatics), what inductive biases are necessary, how to evaluate “competence” vs “performance,” and how to connect model behavior back to human acquisition, typology, and explanation (not just prediction)? 