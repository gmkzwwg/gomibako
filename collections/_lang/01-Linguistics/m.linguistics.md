---
category: Map
title: Basics and Learning Map of Linguistics
tags: Linguistics
---

## Foundations and Conceptual Framework of Theoretical Linguistics


#### Ontology of Linguistics

Language has been conceived in multiple ways: as an innate mental faculty, a social convention, and a biological capacity. Structuralist Ferdinand de Saussure (1916) famously argued that **language (langue)** is neither merely individual speech nor private thought but a public social system of signs; it exists only in the shared conventions of a speech community. In sharp contrast, the generative tradition (Chomsky) treats language as an internal cognitive object. In Chomsky’s view the relevant object of study is the **I‑language** (an individual’s internal grammar) rather than external utterances (“E‑languages”), which are seen as derivative. Philosophers summarize the options as _mentalism_ (language as mental grammar), _platonism_ (language as abstract types or systems), and _nominalism_ (language as sets of utterances in communities). Contemporary linguists often adopt a pluralistic stance: language incorporates mental structures (grammar), social use (communication), and biological underpinnings (human neurocognition). In this view, linguistics is a science that models the abstract rules and representations of language (often as psychological or formal objects) while also describing their physical realizations in speech, writing, and communication across communities.

#### Paradigm Shifts and Brief History

The study of language has undergone several major shifts. In the 19th century **comparative philology** dominated: scholars reconstructed proto-languages (e.g. Proto-Indo-European) by systematic sound correspondences across related tongues. This historical-comparative work (stimulated by William Jones’s discovery of Sanskrit’s kinship to Latin/Greek) laid foundations for **historical linguistics** and typology. In the early 20th century, **structuralism** arose. Saussure’s _Cours_ (1916) inaugurated a synchronic view: language is an abstract system (langue) underlying utterances (parole). European schools (Prague, Hjelmslev) and American contemporaries (Boas, Sapir) developed systematic description of language structure (phonemes, grammar) largely independently in each language or family. In the 1930s–50s, American linguist Leonard Bloomfield codified **American structuralism**: he applied rigorous, behaviorist methods to describe phonology and grammar in unfamiliar languages, largely _excluding_ reference to meaning or mental states. In the 1950s–60s, Noam Chomsky led a **generative revolution**: he argued that linguistics should model the innate, rule-based “competence” underlying all possible utterances. His _Syntactic Structures_ (1957) and _Aspects_ (1965) introduced notions like Universal Grammar, competence vs. performance, and recursive phrase structure. This shifted focus from purely descriptive analysis to formal, explanatory theory.

Since the late 20th century, alternative approaches have gained prominence. **Generative grammar** remains influential but now coexists with **functional** and **cognitive** models. Cognitive linguists (Lakoff, Langacker, Talmy, etc.) view language as governed by general cognitive processes rather than a specialized module. **Functional and usage-based** approaches (Halliday’s systemic-functional grammar, Croft’s Functional Discourse Grammar, Construction Grammar, etc.) emphasize language use, meaning, and communicative function. In particular, **usage-based models** (Bybee, Tomasello, Christiansen, etc.) posit that linguistic structure emerges from usage patterns: language is shaped by frequency, analogy, and embodiment rather than solely by abstract rules. Today’s field is characterized by debates and synthesis: researchers explore how innate structures and learned usage interact. For example, recent work advocates a “third-way” in which an innate grammatical component and usage-driven statistics both shape language.

#### Academic Knowledge Structure

Modern theoretical linguistics is typically organized by _levels of structure_ within language. Core subfields include **phonetics** (physical articulation and acoustics of speech sounds) and **phonology** (the abstract sound system of a language), **morphology** (the internal structure of words and morphemes), **syntax** (rules governing sentence structure and word order), **semantics** (principles of meaning and interpretation), and **pragmatics** (contextual meaning and language use). As one overview notes, “the fields that are generally considered the core of theoretical linguistics are phonology, morphology, syntax, and semantics” (with phonetics usually treated as foundational background). In practice, university curricula often offer courses or modules in each of these areas. For example:

*   **Phonetics:** Studies how speech sounds are produced, transmitted, and perceived (articulatory/acoustic analysis).
    
*   **Phonology:** Deals with the patterns and systems of sounds (phonemes, features, rules of sound alternation). Phonology abstracts from raw acoustics to model the mental sound inventory and its organization.
    
*   **Morphology:** Concerns how words are built from smaller units (morphemes) and how word forms inflect or derive. Morphology examines processes like affixation, compounding, and the typological distinction between analytic vs. synthetic languages.
    
*   **Syntax:** Analyzes sentence structure: how words combine into phrases and clauses, and what hierarchical configurations (constituency, dependencies) underlie grammatical sentences. Modern syntax includes transformational or generative models (phrase-structure rules, movement), as well as alternatives (dependency grammar, HPSG, etc.).
    
*   **Semantics:** Explores meaning composition: how word meanings combine into phrase meanings (compositionality) and how sentences convey truth-conditional or propositional content. Formal semantics uses tools from logic and set theory to model meaning, while lexical semantics studies word meanings and relations (synonymy, polysemy).
    
*   **Pragmatics:** Focuses on language in context – how speakers use language to convey implied meanings, perform speech acts, and reference discourse entities. Topics include implicature (Grice), reference resolution, discourse cohesion, and socio-pragmatic factors.
    

These subfields interact: for instance, morphosyntax links morphology and syntax (agreement, word order constraints), phonology interacts with morphology (allomorphy conditioned by sound), and semantics/pragmatics together explain how utterances make sense in context. (Specialized fields like typology, historical linguistics, psycholinguistics and computational linguistics often build on these core areas.)

#### Core Concepts and Basics

Theoretical linguistics rests on several foundational concepts and distinctions:

*   **Competence vs. Performance:** Chomsky distinguished a speaker’s _competence_ (the internalized knowledge of grammar) from _performance_ (actual language use with errors). Competence is the abstract system; performance is its real-world application. For example, speakers may fail to produce a grammatical sentence due to memory limits, but this does not change what their competence grammar allows.
    
*   **Grammaticality:** Relatedly, linguists distinguish _grammatical_ (well-formed) sentences from _ungrammatical_ ones. Under a competence-based view, a sentence like _“That cats is eating the mouse”_ is ruled ungrammatical because English grammar requires agreement (so “cats” with plural demonstrative _those_). Grammaticality judgments are often elicited by linguists to infer the rules of competence.
    
*   **Recursion:** A key property of human language is _recursive_ structure: linguistic rules can apply to their own output, allowing phrases to nest indefinitely (e.g. “the cat \[that the dog \[that the man fed\] chased\] meowed”). Recursion underlies the infinite generativity of language – the fact that finite means yield an unbounded number of sentences.
    
*   **Phoneme:** The phoneme is the smallest contrastive sound unit. For example, /p/ and /b/ are different phonemes in “pat” vs. “bat.” A phoneme may have multiple **allophones** (contextual variants) that do not change meaning (e.g. the aspirated \.
    
*   **Morpheme:** The morpheme is the smallest grammatical unit of meaning or function. It can be a free word (e.g. _cat_, _walk_) or a bound element (e.g. English plural _-s_, past tense _-ed_). A word like _“cats”_ contains two morphemes: _cat_ \+ _-s_. Morphemes often have **allomorphs**, variant forms conditioned by context (e.g. _-s_, _-es_, or irregular forms all marking plural).
    
*   **Constituency:** In syntax, sentences are analyzed in terms of **constituents** (phrases). For example, “the big dog” is a noun phrase constituent. Constituency tests (substitution, movement) reveal how words group into hierarchical structures, a cornerstone of phrase-structure grammar.
    
*   **Compositionality:** A fundamental semantic principle (Frege’s principle) is that the meaning of a complex expression is determined by the meanings of its parts and the way they are combined. This explains how we understand novel sentences: we compute the sense of an unfamiliar sentence by combining known word meanings according to grammatical rules.
    
*   **Reference and Meaning:** Linguists distinguish _reference_ (the real-world entities or concepts a term refers to) from _sense_ (conceptual meaning). For example, the word “dog” refers to the set of all dogs but has a sense that distinguishes it from “cat.” Semantics also deals with relations like synonymy, antonymy, and entailment.
    
*   **Universals and Typology:** Linguists seek _universals_ – properties or parameters common to all languages – and use typological classification to organize languages by structural patterns (word order, morphological type, phoneme inventories, etc.). Iconic universals (Greenbergian universals) or functional tendencies (e.g. Subject–Object–Verb order) have been extensively catalogued, reflecting deep constraints or preferences in human language.
    
*   **Markedness:** This concept (originating in Prague School phonology) captures asymmetry: in a contrasting pair, one element is _unmarked_ (the default) and the other _marked_ (more complex or specific). For example, singular _“cat”_ is unmarked, while plural _“cats”_ is marked (by the suffix). Markedness has been applied to sounds (voiced vs. voiceless), morphology (tense distinctions), and syntax (active vs. passive), often correlating with cognitive or frequency factors.
    

These and other core ideas – e.g. _feature geometry_ in phonology, _parameter-setting_ in syntax, _frame semantics_ in lexicon – form the toolkit of theoretical linguists. Mastery of these concepts and the ability to apply them within formal models (rules, tree diagrams, logical formulas) is essential for advanced study.

#### Conclusion and Further Topics

The frontier of theoretical linguistics spans many intersecting domains. **Linguistic typology and universals** continue to broaden our understanding of language diversity and limits (with large-scale databases like WALS). **Language acquisition** (first language, second language) connects theory to development: how children infer grammar from input is a major research focus, engaging psycholinguistics and computational modeling. **Neurolinguistics** probes the brain basis of language (e.g. Broca’s and Wernicke’s areas for production/comprehension, neural oscillations in syntax) using imaging and lesion studies. In **formal semantics**, researchers develop richer models (possible worlds, situation semantics) and interfaces with pragmatics (e.g. modeling implicatures and presuppositions).

Important debates persist: _nativist_ versus _usage-based_ accounts of grammar (Chomsky vs. cognitive-functionalists), the modularity of mind versus embodied cognition, and the balance between formal description and communicative function. Recent work, for instance, argues that both innate structure and statistical learning contribute to language. In **philosophy of language**, questions of reference, truth, and meaning (from Frege and Tarski to contemporary pragmatic theories) remain central to linguistic semantics and grammar.

Emerging tools and domains are reshaping the field. **Corpus linguistics** and computational methods allow empirically-driven discoveries across millions of utterances; **machine learning** and NLP raise new questions about modeling competence. **Evolutionary linguistics** investigates how language may have emerged biologically (genes like FOXP2, vocal tract changes, comparative primate studies). **Interdisciplinary work** ties linguistics to genetics, neuroscience, anthropology, and artificial intelligence. Together, these topics ensure theoretical linguistics remains a vibrant field, continually refining its models of what language is and how it works at all levels of structure and use.

## Casual Chain of the Development of Linguistics

Below is a compact “question → solution/argument → next question” chain (with approximate dates). It is selective (you could build parallel chains for Chinese philology, Arabic grammar, indigenous-language documentation, etc.), but it covers the standard academic narrative plus a few major alternatives.


#### 1) Rule-system grammar (ancient South Asia; mid–1st millennium BCE)

**Question:** How can a prestigious language variety be specified *exactly* (so it can be taught, reproduced, and kept stable)?
**Resolution (method):** A highly formal rule system with meta-rules that *generates/licences* well-formed forms (a compact algorithmic grammar). ([OUP Academic][1])
**Associated argument/move:** Treat grammar as an explicit rule calculus with economy and coverage as design pressures. ([OUP Academic][1])
**Next question produced:** If grammar can be formal, are there *general* principles shared across languages, or links between grammar and thought?


#### 2) General/rational grammar (Europe; 1660s)

**Question:** What aspects of grammar reflect universal structures of thought/logic rather than accidents of a particular language?
**Resolution (method):** “General and rational” grammar: analyze grammar through predication/logic and cross-linguistic comparison (Latin/Greek/French, etc.). ([ウィキペディア][2])
**Associated argument/move:** Similarities across languages arise because there is (roughly) one logic underlying thought. ([ウィキペディア][2])
**Next question produced:** But languages *change* and diverge—how do we explain historical relatedness and regular change?


#### 3) Comparative–historical linguistics and the “sound law” program (Europe; 19th century; “Neogrammarians” late 1800s)

**Question:** Can language change be studied with the rigor of a science (regularities, prediction, falsification)?
**Resolution (method):** Comparative method plus the Neogrammarian regularity hypothesis: sound change is (in principle) exceptionless; apparent exceptions are handled via conditioning environments and analogy. ([ウィキペディア][3])
**Associated argument/move:** Treat sound change as lawlike; separate phonetic regularity from morphological leveling (analogy). ([bas.uni-muenchen.de][4])
**Next question produced:** Historical rigor is not enough—what is the proper object of study for describing a language *as a system* at a time?


#### 4) Structuralism (Europe; early 20th century; Saussurean turn)

**Question:** What is “language” as an object—individual utterances, or an abstract system enabling them? How should we separate synchrony (system-at-a-time) from diachrony (change)?
**Resolution (method):** Define the linguistic object as a structured system (langue) distinct from speech (parole); make synchronic structure primary as a scientific target. ([ia600204.us.archive.org][5])
**Associated argument/move:** The sign and the network of relations are central; structure is not reducible to a list of words. ([ia600204.us.archive.org][5])
**Next question produced:** How do we build *replicable* methods to discover structure from data—especially for underdescribed languages?


#### 5) American structuralism / distributionalism (U.S.; 1930s–1950s)

**Question:** Can linguistic analysis be made operational and data-driven without relying on introspection or meaning (which seemed “unscientific” to some)?
**Resolution (method):** Distributional analysis and “discovery procedure” ideals: infer categories/structures from observable patterns of occurrence. ([ウィキペディア][6])
**Associated argument/move:** Syntax can be grounded in distributional facts; meaning can be bracketed (at least initially) to keep procedures objective. ([ウィキペディア][6])
**Next question produced:** But humans produce and understand *novel* sentences—what kind of internal system explains productivity and acquisition?


#### 6) Generative grammar (1957 onward)

**Question:** How can finite resources yield an unbounded set of grammatical sentences, and what must a learner “bring” to acquisition?
**Resolution (method):** A grammar is a generative device that produces all and only the grammatical sequences; model competence as an internal system. ([danielwharris.com][7])
**Associated argument/move:** Shift from “discovery procedures” to explicit theories evaluated by explanatory adequacy (including learnability considerations). ([danielwharris.com][7])
**Next question produced:** How do meaning and use connect to syntax—especially quantification, context-dependence, and inference beyond truth conditions?


#### 7) Formal semantics (1970s; Montague)

**Question:** Can natural language meaning be treated with the same formal precision as logic, compositionally, including quantification?
**Resolution (method):** Model natural-language syntax–semantics with higher-order logic and lambda calculus; treat natural and formal languages in a unified framework. ([ウィキペディア][8])
**Associated argument/move:** Reject a sharp theoretical divide between NL and formal systems (as a research stance); insist on compositionality and explicit models. ([ウィキペディア][8])
**Next question produced:** Meaning in interaction often exceeds compositional truth conditions—how do actions, intentions, and conversational reasoning enter the theory?


#### 8) Pragmatics: speech acts + implicature (1960s–1970s; Austin, Grice)

**Question:** What is it to *do* things with words (promise, baptize, order), and how do hearers infer unstated content reliably?
**Resolution (method):** Speech act framework (felicity conditions, illocutionary force) and conversational implicature via cooperative principles/maxims. ([MPG.PuRe][9])
**Associated argument/move:** Not all meaning is truth-conditional; inference is systematic and partly rule-governed by interactional norms. ([Summer School on Law and Logic 2023][10])
**Next question produced:** If use and inference matter, then variation across speakers and social settings is not “noise”—how is it structured, and how does it drive change?


#### 9) Variationist sociolinguistics (1960s onward; Labovian program)

**Question:** Is “free variation” real, or is variation patterned by social and linguistic constraints—and does it reveal change in progress?
**Resolution (method):** Quantitative analysis of sociolinguistic variables correlated with social factors; model variation as structured rather than accidental. ([Wiley-Blackwell][11])
**Associated argument/move:** Empirical field methods + statistics can turn variation into evidence about grammar, diffusion, and change. ([Wiley-Blackwell][11])
**Next question produced:** Beyond single communities, what constraints and tendencies hold across languages globally (typology/universals), and what explains them?


#### 10) Typology and universals (mid-20th century onward; Greenbergian tradition)

**Question:** What cross-linguistic regularities exist (e.g., word-order correlations), and are they accidental, functional, or cognitive?
**Resolution (method):** Large comparative sampling; formulate implicational universals and correlations from attested diversity. ([インターネットアーカイブ][12])
**Associated argument/move:** Universals can be empirical generalizations over many languages (not only theory-internal claims). ([インターネットアーカイブ][12])
**Next question produced:** If grammar is shaped by function and use, can we model grammar as a resource for meaning-in-context rather than primarily as formal derivation?


#### 11) Functional / systemic-functional linguistics (mid–late 20th century; Halliday)

**Question:** How does language realize social action and meaning across contexts (register/genre), not merely structure?
**Resolution (method):** Model language as a social semiotic system organized around choices (systems), with context-sensitive functional dimensions. ([ウィキペディア][13])
**Associated argument/move:** Prioritize meaning and function (ideational/interpersonal/textual) as explanatory for linguistic form. ([ウィキペディア][13])
**Next question produced:** If usage and function matter, are “rules” the right primitives—or are learned form–meaning pairings (“constructions”) and frequency effects more basic?


#### 12) Construction grammar, cognitive and usage-based approaches (1980s–2000s+)

**Question:** How are grammar and meaning linked at the level of recurring patterns, and how do frequency and experience shape representation and learning?
**Resolution (method):** Constructions as form–meaning pairings (including argument structure); conceptual metaphor as part of cognition; usage-based learning with frequency effects; usage-based acquisition theories. ([University of Chicago Press][14])
**Associated argument/move:** Grammar emerges from use and general cognition rather than requiring a sharply separate autonomous module (strong versions vary by author). ([terpconnect.umd.edu][15])
**Next question produced:** How can we formalize gradient well-formedness and competing pressures, especially in phonology?

#### 13) Constraint-based optimization in phonology (1990s; Optimality Theory)

**Question:** How can we model surface patterns as the result of competing constraints, including cross-linguistic variation?
**Resolution (method):** Ranked, violable constraints evaluated over candidate outputs (optimization). ([ウィキペディア][16])
**Associated argument/move:** Replace many ordered rules with constraint interaction + language-specific rankings. ([ウィキペディア][16])
**Next question produced:** With expanding data and computation, how do we scale evidence and evaluation beyond hand-picked examples?

#### 14) Corpus linguistics (late 20th century onward)

**Question:** How can linguistic claims be tested and discovered using large-scale real usage data?
**Resolution (method):** Computer-aided analysis of large corpora; corpus evidence used to test, refine, or sometimes drive generalizations. ([Cambridge University Press & Assessment][17])
**Associated argument/move:** Methodological shift toward reproducibility and broad coverage; stronger emphasis on variation, frequency, collocation, and distribution. ([Cambridge University Press & Assessment][17])
**Next question produced:** Can computational models learn linguistic structure/meaning from data at scale—and what does that imply about theory?

#### 15) Neural NLP and the “foundation model” era (2017– )

**Question:** Can a general architecture learn high-quality language representations and transfer across tasks, and what mechanisms support scaling?
**Resolution (method):** Transformer architecture based on attention; large-scale pretraining (e.g., BERT) and task adaptation. ([arxiv.org][18])
**Associated argument/move:** Replace heavy task-specific feature engineering with representation learning and scaling laws; attention enables parallelism and long-range dependencies. ([arxiv.org][18])
**Next question produced (current cycle):** What, if anything, do such models *know* in linguistically interpretable terms (syntax/semantics/pragmatics), what inductive biases are necessary, how to evaluate “competence” vs “performance,” and how to connect model behavior back to human acquisition, typology, and explanation (not just prediction)? ([arxiv.org][19])

[1]: https://academic.oup.com/book/41916/chapter/354803828?utm_source=chatgpt.com "Pāṇini | The Oxford History of Phonology"
[2]: https://en.wikipedia.org/wiki/Port-Royal_Grammar?utm_source=chatgpt.com "Port-Royal Grammar"
[3]: https://en.wikipedia.org/wiki/Neogrammarian?utm_source=chatgpt.com "Neogrammarian"
[4]: https://www.bas.uni-muenchen.de/~jmh/lehre/sem/ws2425/lectures/lecture%201/neogrammarian.pdf?utm_source=chatgpt.com "lecture 3 NEOGRAMMARIAN SOUND CHANGE"
[5]: https://ia600204.us.archive.org/0/items/SaussureFerdinandDeCourseInGeneralLinguistics1959/Saussure_Ferdinand_de_Course_in_General_Linguistics_1959.pdf?utm_source=chatgpt.com "Course in general linguistics"
[6]: https://en.wikipedia.org/wiki/Distributionalism?utm_source=chatgpt.com "Distributionalism"
[7]: https://danielwharris.com/teaching/364/readings/ChomskySyntactic.pdf?utm_source=chatgpt.com "Noam Chomsky: Syntactic Structures"
[8]: https://en.wikipedia.org/wiki/Montague_grammar?utm_source=chatgpt.com "Montague grammar"
[9]: https://pure.mpg.de/rest/items/item_2271128/component/file_2271430/content?utm_source=chatgpt.com "How to do things with Words"
[10]: https://lawandlogic.org/wp-content/uploads/2018/07/grice1975logic-and-conversation.pdf?utm_source=chatgpt.com "H. P. Grice Logic and Conversation"
[11]: https://www.blackwellpublishing.co.uk/content/BPL_Images/Content_Store/WWW_Content/9780631234944/004.pdf?utm_source=chatgpt.com "4 Variationist Sociolinguistics"
[12]: https://archive.org/download/universalsoflang00unse/universalsoflang00unse_bw.pdf?utm_source=chatgpt.com "Universals of language"
[13]: https://en.wikipedia.org/wiki/Systemic_functional_linguistics?utm_source=chatgpt.com "Systemic functional linguistics"
[14]: https://press.uchicago.edu/ucp/books/book/chicago/C/bo3683810.html?utm_source=chatgpt.com "A Construction Grammar Approach to Argument Structure"
[15]: https://terpconnect.umd.edu/~israel/Bybee_Plenary.pdf?utm_source=chatgpt.com "From Usage to Grammar:"
[16]: https://en.wikipedia.org/wiki/Optimality_theory?utm_source=chatgpt.com "Optimality theory"
[17]: https://www.cambridge.org/core/books/corpus-linguistics/16CC177EA9B6007B3187C64326F818AB?utm_source=chatgpt.com "Corpus Linguistics"
[18]: https://arxiv.org/abs/1706.03762?utm_source=chatgpt.com "Attention Is All You Need"
[19]: https://arxiv.org/abs/1810.04805?utm_source=chatgpt.com "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
